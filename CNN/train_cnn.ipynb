{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c2eb0f-2ac8-4bcb-b598-9fc5fe84cce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140.0 90.0\n",
      "S--epoch:0    MAE:11.001519789123535   Pure:11.001519789123535\n",
      "tensor([[[114.1146]],\n",
      "\n",
      "        [[105.8986]],\n",
      "\n",
      "        [[112.5030]],\n",
      "\n",
      "        [[118.6131]],\n",
      "\n",
      "        [[102.5122]],\n",
      "\n",
      "        [[118.3356]],\n",
      "\n",
      "        [[111.1761]],\n",
      "\n",
      "        [[107.8323]],\n",
      "\n",
      "        [[124.7865]],\n",
      "\n",
      "        [[130.6015]],\n",
      "\n",
      "        [[100.8813]],\n",
      "\n",
      "        [[116.0312]],\n",
      "\n",
      "        [[104.4879]],\n",
      "\n",
      "        [[111.9352]],\n",
      "\n",
      "        [[118.0172]],\n",
      "\n",
      "        [[104.9265]]], device='cuda:0')\n",
      "tensor([[[113.2422]],\n",
      "\n",
      "        [[128.1368]],\n",
      "\n",
      "        [[110.7046]],\n",
      "\n",
      "        [[106.2089]],\n",
      "\n",
      "        [[119.1779]],\n",
      "\n",
      "        [[108.5691]],\n",
      "\n",
      "        [[113.2340]],\n",
      "\n",
      "        [[111.6820]],\n",
      "\n",
      "        [[131.0170]],\n",
      "\n",
      "        [[124.4271]],\n",
      "\n",
      "        [[114.0208]],\n",
      "\n",
      "        [[102.9040]],\n",
      "\n",
      "        [[ 93.2232]],\n",
      "\n",
      "        [[111.0470]],\n",
      "\n",
      "        [[105.4207]],\n",
      "\n",
      "        [[116.2372]]], device='cuda:0')\n",
      "tensor([[[101.9567]],\n",
      "\n",
      "        [[115.1981]],\n",
      "\n",
      "        [[105.6827]],\n",
      "\n",
      "        [[105.5442]],\n",
      "\n",
      "        [[117.9307]],\n",
      "\n",
      "        [[119.5089]],\n",
      "\n",
      "        [[101.5526]],\n",
      "\n",
      "        [[117.0490]],\n",
      "\n",
      "        [[116.2833]],\n",
      "\n",
      "        [[125.1965]],\n",
      "\n",
      "        [[121.0112]],\n",
      "\n",
      "        [[ 96.5357]],\n",
      "\n",
      "        [[117.0611]],\n",
      "\n",
      "        [[104.4335]],\n",
      "\n",
      "        [[120.2822]],\n",
      "\n",
      "        [[108.2172]]], device='cuda:0')\n",
      "tensor([[[112.0524]],\n",
      "\n",
      "        [[119.0769]],\n",
      "\n",
      "        [[103.1827]],\n",
      "\n",
      "        [[112.8004]],\n",
      "\n",
      "        [[116.4938]],\n",
      "\n",
      "        [[112.8638]],\n",
      "\n",
      "        [[117.7171]],\n",
      "\n",
      "        [[109.7653]],\n",
      "\n",
      "        [[120.6639]],\n",
      "\n",
      "        [[112.1419]],\n",
      "\n",
      "        [[101.9978]],\n",
      "\n",
      "        [[117.1956]],\n",
      "\n",
      "        [[110.9238]],\n",
      "\n",
      "        [[ 99.0215]],\n",
      "\n",
      "        [[115.1756]],\n",
      "\n",
      "        [[107.7423]]], device='cuda:0')\n",
      "tensor([[[126.5581]],\n",
      "\n",
      "        [[104.8598]],\n",
      "\n",
      "        [[112.6413]],\n",
      "\n",
      "        [[100.8547]],\n",
      "\n",
      "        [[ 97.9902]],\n",
      "\n",
      "        [[128.3007]],\n",
      "\n",
      "        [[114.5926]],\n",
      "\n",
      "        [[107.7966]],\n",
      "\n",
      "        [[120.5082]],\n",
      "\n",
      "        [[120.9533]],\n",
      "\n",
      "        [[106.6367]],\n",
      "\n",
      "        [[101.6546]],\n",
      "\n",
      "        [[117.3428]],\n",
      "\n",
      "        [[102.2283]],\n",
      "\n",
      "        [[114.9005]],\n",
      "\n",
      "        [[ 97.8941]]], device='cuda:0')\n",
      "tensor([[[115.7059]],\n",
      "\n",
      "        [[118.0417]],\n",
      "\n",
      "        [[112.7289]],\n",
      "\n",
      "        [[116.1950]],\n",
      "\n",
      "        [[114.8378]],\n",
      "\n",
      "        [[120.2045]],\n",
      "\n",
      "        [[127.6741]],\n",
      "\n",
      "        [[113.4232]],\n",
      "\n",
      "        [[ 96.5189]],\n",
      "\n",
      "        [[103.6749]],\n",
      "\n",
      "        [[114.3961]],\n",
      "\n",
      "        [[106.5926]],\n",
      "\n",
      "        [[110.0475]],\n",
      "\n",
      "        [[ 94.8205]],\n",
      "\n",
      "        [[127.5653]],\n",
      "\n",
      "        [[110.7588]]], device='cuda:0')\n",
      "tensor([[[113.9620]],\n",
      "\n",
      "        [[119.0868]],\n",
      "\n",
      "        [[104.5272]],\n",
      "\n",
      "        [[106.0141]],\n",
      "\n",
      "        [[104.5553]],\n",
      "\n",
      "        [[106.3997]],\n",
      "\n",
      "        [[107.6760]],\n",
      "\n",
      "        [[115.3696]],\n",
      "\n",
      "        [[116.1533]],\n",
      "\n",
      "        [[105.0651]],\n",
      "\n",
      "        [[101.2269]],\n",
      "\n",
      "        [[118.3068]],\n",
      "\n",
      "        [[106.6029]],\n",
      "\n",
      "        [[127.9421]],\n",
      "\n",
      "        [[127.5135]],\n",
      "\n",
      "        [[106.7071]]], device='cuda:0')\n",
      "tensor([[[117.2649]],\n",
      "\n",
      "        [[106.7089]],\n",
      "\n",
      "        [[105.0358]],\n",
      "\n",
      "        [[122.9173]],\n",
      "\n",
      "        [[105.7868]],\n",
      "\n",
      "        [[125.0966]],\n",
      "\n",
      "        [[110.9942]],\n",
      "\n",
      "        [[111.0714]],\n",
      "\n",
      "        [[103.1038]],\n",
      "\n",
      "        [[111.2283]],\n",
      "\n",
      "        [[112.8050]],\n",
      "\n",
      "        [[118.1167]],\n",
      "\n",
      "        [[104.6142]],\n",
      "\n",
      "        [[123.0032]],\n",
      "\n",
      "        [[108.9123]],\n",
      "\n",
      "        [[118.8889]]], device='cuda:0')\n",
      "tensor([[[112.2083]],\n",
      "\n",
      "        [[113.8012]],\n",
      "\n",
      "        [[119.5935]],\n",
      "\n",
      "        [[118.5609]],\n",
      "\n",
      "        [[103.7795]],\n",
      "\n",
      "        [[124.3604]],\n",
      "\n",
      "        [[114.7765]],\n",
      "\n",
      "        [[112.5867]],\n",
      "\n",
      "        [[109.3158]],\n",
      "\n",
      "        [[104.7995]],\n",
      "\n",
      "        [[103.8000]],\n",
      "\n",
      "        [[118.6582]],\n",
      "\n",
      "        [[106.1626]],\n",
      "\n",
      "        [[121.6026]],\n",
      "\n",
      "        [[108.4303]],\n",
      "\n",
      "        [[115.7120]]], device='cuda:0')\n",
      "tensor([[[ 96.6980]],\n",
      "\n",
      "        [[108.3673]],\n",
      "\n",
      "        [[104.1774]],\n",
      "\n",
      "        [[100.2963]],\n",
      "\n",
      "        [[106.3340]],\n",
      "\n",
      "        [[114.1956]],\n",
      "\n",
      "        [[123.9381]],\n",
      "\n",
      "        [[105.4187]],\n",
      "\n",
      "        [[111.7135]],\n",
      "\n",
      "        [[119.7422]],\n",
      "\n",
      "        [[108.8604]],\n",
      "\n",
      "        [[110.8178]],\n",
      "\n",
      "        [[120.8649]],\n",
      "\n",
      "        [[123.0542]],\n",
      "\n",
      "        [[108.9726]],\n",
      "\n",
      "        [[120.8078]]], device='cuda:0')\n",
      "tensor([[[109.8081]],\n",
      "\n",
      "        [[108.9836]],\n",
      "\n",
      "        [[100.6315]],\n",
      "\n",
      "        [[122.8531]],\n",
      "\n",
      "        [[128.8302]],\n",
      "\n",
      "        [[ 91.4298]],\n",
      "\n",
      "        [[121.5668]],\n",
      "\n",
      "        [[110.2566]],\n",
      "\n",
      "        [[110.6917]],\n",
      "\n",
      "        [[119.0063]],\n",
      "\n",
      "        [[105.8133]],\n",
      "\n",
      "        [[102.9108]],\n",
      "\n",
      "        [[120.1688]],\n",
      "\n",
      "        [[112.7556]],\n",
      "\n",
      "        [[118.5227]],\n",
      "\n",
      "        [[115.1259]]], device='cuda:0')\n",
      "tensor([[[103.7089]],\n",
      "\n",
      "        [[110.8865]],\n",
      "\n",
      "        [[107.8452]],\n",
      "\n",
      "        [[124.2617]],\n",
      "\n",
      "        [[114.3942]],\n",
      "\n",
      "        [[114.5038]],\n",
      "\n",
      "        [[115.8345]],\n",
      "\n",
      "        [[111.2857]],\n",
      "\n",
      "        [[109.9751]],\n",
      "\n",
      "        [[123.5360]],\n",
      "\n",
      "        [[106.8743]],\n",
      "\n",
      "        [[106.8898]],\n",
      "\n",
      "        [[119.8433]],\n",
      "\n",
      "        [[105.0429]],\n",
      "\n",
      "        [[103.1434]],\n",
      "\n",
      "        [[ 98.1135]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[1;32m     64\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda(), target\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 65\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# inv_norm\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# output = output * (max - min) + min\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/autodl-nas/AutoDL/CNN/model.py:212\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    211\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m--> 212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n\u001b[1;32m    214\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:302\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:298\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    296\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    297\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from dataset import *\n",
    "from model import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_log = np.array([])\n",
    "test_log = np.array([])\n",
    "lambda_l1 = 0\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "model = VGG().cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "tmp = np.load(\"../data/simu_20000_0.1_90_140_train.npy\")\n",
    "max = np.max(tmp[:, 1004])\n",
    "min = np.min(tmp[:, 1004])\n",
    "print(max, min)\n",
    "\n",
    "train_dataset = Dataset(\"../data/simu_20000_0.1_90_140_train.npy\", 0, 0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataset = Dataset(\"../data/simu_10000_0.1_90_140_resonance_morlet.npy\", 0, 1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "\n",
    "    loss_total = 0\n",
    "    step = 0\n",
    "    \n",
    "    loss_pure = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss_pure = loss_pure + loss.item()\n",
    "        \n",
    "        l1_regularization = torch.tensor(0.0).cuda()\n",
    "        for param in model.parameters():\n",
    "            l1_regularization += torch.norm(param, p=1)\n",
    "        loss += l1_regularization * lambda_l1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_total = loss_total + loss.item()\n",
    "        step = step + 1\n",
    "\n",
    "    tmp = './pth/S_model_%d_%.4f.pth' % (epoch, loss_total/step)\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model, tmp)\n",
    "    print(\"S--epoch:\" + str(epoch) + \"    MAE:\" + str(loss_total/step) + \"   Pure:\" + str(loss_pure/step))\n",
    "    train_log = np.append(train_log, loss_total/step)\n",
    "\n",
    "    loss_test = 0\n",
    "    step = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            print(output)\n",
    "\n",
    "            # inv_norm\n",
    "            # output = output * (max - min) + min\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            loss_test = loss_test + loss.item()\n",
    "            step = step + 1\n",
    "\n",
    "        loss_mean = loss_test / step\n",
    "        print(\"epoch:\" + str(epoch) + \"    MAE_test:\" + str(loss_mean))\n",
    "        test_log = np.append(test_log, loss_mean)\n",
    "    \n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        print(\"Test_epoch:\" + str(epoch) + \"    MAE_test:\" + str(loss_mean))\n",
    "        # tmp_epoch = np.arange(epoch+1)\n",
    "        # print(tmp_epoch)\n",
    "        # print(next(model.OneToOneLayer.parameters()))\n",
    "        plt.plot(train_log)\n",
    "        plt.plot(test_log)\n",
    "        plt.ylim(0,20)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
